{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78444f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\lenovo -pc\\anaconda3\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lenovo -pc\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lenovo -pc\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo -pc\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo -pc\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo -pc\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo -pc\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo -pc\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be7221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Rating, Year of Release]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "movies = soup.find_all('div', class_='lister-item mode-detail')\n",
    "\n",
    "names = []\n",
    "ratings = []\n",
    "years = []\n",
    "\n",
    "for movie in movies:\n",
    "    name = movie.h3.a.text.strip()\n",
    "    names.append(name)\n",
    "    \n",
    "    rating = movie.find('span', class_='star__rating').text.strip()\n",
    "    ratings.append(rating)\n",
    "    \n",
    "    year = movie.find('span', class_='item-year').text.strip('() ')\n",
    "    years.append(year)\n",
    "\n",
    "movies_df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'Rating': ratings,\n",
    "    'Year of Release': years\n",
    "})\n",
    "\n",
    "print(movies_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e11ad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Heading, Date, Content, Likes, YouTube Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.patreon.com/coreyms\"\n",
    "\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_, 'html.parser')\n",
    "\n",
    "headings = []\n",
    "dates = []\n",
    "contents = []\n",
    "likes = []\n",
    "youtube_links = []\n",
    "\n",
    "posts = soup.find_all('div', class_='scaffold')\n",
    "\n",
    "for post in posts:\n",
    "    heading = post.find('h2', class_='scaffold-title').text.strip()\n",
    "    headings.append(heading)\n",
    "    \n",
    "    date = post.find('time').text.strip()\n",
    "    dates.append(date)\n",
    "    \n",
    "    content = post.find('div', class_='scaffold-content').text.strip()\n",
    "    contents.append(content)\n",
    "    \n",
    "    like = post.find('span', class_='like-count').text.strip() if post.find('span', class_='like-count') else 'N/A'\n",
    "    likes.append(like)\n",
    "    \n",
    "    youtube_link = post.find('a', href=True)\n",
    "    youtube_link = youtube_link['href'] if youtube_link and 'youtube.com' in youtube_link['href'] else 'N/A'\n",
    "    youtube_links.append(youtube_link)\n",
    "\n",
    "posts_df = pd.DataFrame({\n",
    "    'Heading': headings,\n",
    "    'Date': dates,\n",
    "    'Content': contents,\n",
    "    'Likes': likes,\n",
    "    'YouTube Link': youtube_links\n",
    "})\n",
    "\n",
    "print(posts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f13777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Location, Area, EMI, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_details(locality):\n",
    "    url = f\"https://www.nobroker.in/property/sale/{locality.replace(' ', '-')}?searchParam=W3sibGF0IjoxMi45NzQyOTQwLCJsb24iOjc3LjYzMTQ4NDAsInBsYWNlSWQiOiJDaElKS0l0QWh1U3NYVWNSb0JlVnUtVEh4aHciLCJwbGFjZU5hbWUiOiJJbmRpcmEgTmFnYXIifV0=&radius=2.0&sharedAccomodation=0&city=bangalore\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    titles = []\n",
    "    locations = []\n",
    "    areas = []\n",
    "    emis = []\n",
    "    prices = []\n",
    "\n",
    "    houses = soup.find_all('div', class_='card-header-title')\n",
    "\n",
    "    for house in houses:\n",
    "        title = house.find('h2').text.strip()\n",
    "        titles.append(title)\n",
    "\n",
    "        location = house.find('div', class_='card-header-title-location').text.strip()\n",
    "        locations.append(location)\n",
    "\n",
    "        area = house.find('div', class_='nb__3oNyC').text.strip() if house.find('div', class_='nb__3oNyC') else 'N/A'\n",
    "        areas.append(area)\n",
    "\n",
    "        emi = house.find('div', class_='nb__5MTV2').text.strip() if house.find('div', class_='nb__5MTV2') else 'N/A'\n",
    "        emis.append(emi)\n",
    "\n",
    "        price = house.find('div', class_='nb__19hcF').text.strip() if house.find('div', class_='nb__19hcF') else 'N/A'\n",
    "        prices.append(price)\n",
    "\n",
    "    houses_df = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Location': locations,\n",
    "        'Area': areas,\n",
    "        'EMI': emis,\n",
    "        'Price': prices\n",
    "    })\n",
    "\n",
    "    return houses_df\n",
    "\n",
    "localities = ['Indira Nagar', 'Jayanagar', 'Rajaji Nagar']\n",
    "\n",
    "house_df = pd.DataFrame()\n",
    "\n",
    "for locality in localities:\n",
    "    houses_df = get_details(locality)\n",
    "    house_df = pd.concat([house_df, houses_df], ignore_index=True)\n",
    "\n",
    "print(house_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4baf9620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Product Name  Price                                          Image URL\n",
      "0        Bewakoof®   ₹629  https://images.bewakoof.com/t640/women-s-green...\n",
      "1        Bewakoof®   ₹899  https://images.bewakoof.com/t640/jet-black-plu...\n",
      "2        Bewakoof®   ₹709  https://images.bewakoof.com/t640/men-s-purple-...\n",
      "3        Bewakoof®   ₹999  https://images.bewakoof.com/t640/navy-blue-plu...\n",
      "4        Bewakoof®   ₹509  https://images.bewakoof.com/t640/women-s-purpl...\n",
      "5        Bewakoof®   ₹399  https://images.bewakoof.com/t640/men-s-black-a...\n",
      "6        Bewakoof®   ₹679  https://images.bewakoof.com/t640/men-s-black-o...\n",
      "7  bewakoof x nasa   ₹599  https://images.bewakoof.com/t640/men-s-red-spa...\n",
      "8        Bewakoof®   ₹449  https://images.bewakoof.com/t640/women-s-pink-...\n",
      "9        Bewakoof®  ₹1299  https://images.bewakoof.com/t640/women-s-blue-...\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "product_names = []\n",
    "prices = []\n",
    "image_urls = []\n",
    "\n",
    "products = soup.find_all('div', class_='productCardBox', limit=10)\n",
    "\n",
    "for product in products:\n",
    "    name = product.find('div', class_='productCardDetail').find('h3').text.strip()\n",
    "    product_names.append(name)\n",
    "    \n",
    "    price = product.find('div', class_='discountedPriceText').text.strip()\n",
    "    prices.append(price)\n",
    "    \n",
    "    image_url = product.find('img', class_='productImgTag')['src']\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "products_df = pd.DataFrame({\n",
    "    'Product Name': product_names,\n",
    "    'Price': prices,\n",
    "    'Image URL': image_urls\n",
    "})\n",
    "\n",
    "print(products_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00df1d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m headings\u001b[38;5;241m.\u001b[39mappend(heading)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Extract the article date\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m date \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCard-time\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     32\u001b[0m dates\u001b[38;5;241m.\u001b[39mappend(date)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Extract the news link\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#5\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "headings = []\n",
    "dates = []\n",
    "news_links = []\n",
    "\n",
    "articles = soup.find_all('div', class_='Card-standardBreakerCard')\n",
    "\n",
    "for article in articles:\n",
    "    heading = article.find('a', class_='Card-title').text.strip()\n",
    "    headings.append(heading)\n",
    "    \n",
    "    date = article.find('time', class_='Card-time')['datetime']\n",
    "    dates.append(date)\n",
    "    \n",
    "    news_link = article.find('a', class_='Card-title')['href']\n",
    "    news_links.append(news_link)\n",
    "\n",
    "news_df = pd.DataFrame({\n",
    "    'Heading': headings,\n",
    "    'Date': dates,\n",
    "    'News Link': news_links\n",
    "})\n",
    "\n",
    "print(news_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a68559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Date, Author]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "titles = []\n",
    "dates = []\n",
    "authors = []\n",
    "\n",
    "articles = soup.find_all('div', class_='flex-container article-entry')\n",
    "\n",
    "for article in articles:\n",
    "    title = article.find('h2').text.strip()\n",
    "    titles.append(title)\n",
    "    \n",
    "    date = article.find('div', class_='text-xs').text.strip().split('|')[1].strip()\n",
    "    dates.append(date)\n",
    "    \n",
    "    author = article.find('div', class_='text-xs').text.strip().split('|')[0].strip()\n",
    "    authors.append(author)\n",
    "    \n",
    "articles_df = pd.DataFrame({\n",
    "    'Paper Title': titles,\n",
    "    'Date': dates,\n",
    "    'Author': authors\n",
    "})\n",
    "\n",
    "print(articles_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be78a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
