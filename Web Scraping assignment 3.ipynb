{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "062a8820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search: guitar\n",
      "Failed to retrieve search results. Status code: 503\n",
      "No products found.\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_products(search_query):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f'https://www.amazon.in/s?k={search_query.replace(\" \", \"+\")}'\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve search results. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    products = []\n",
    "\n",
    "    for item in soup.find_all('div', {'data-component-type': 's-search-result'}):\n",
    "        title = item.h2.text.strip() if item.h2 else 'No title'\n",
    "        link = f\"https://www.amazon.in{item.h2.a['href']}\" if item.h2 and item.h2.a else 'No link'\n",
    "        price = item.find('span', 'a-price-whole')\n",
    "        price = price.text if price else 'No price'\n",
    "\n",
    "        products.append({\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'price': price\n",
    "        })\n",
    "\n",
    "    return products\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the product to search: \")\n",
    "    products = get_products(search_query)\n",
    "\n",
    "    if products:\n",
    "        for idx, product in enumerate(products, 1):\n",
    "            print(f\"{idx}. {product['title']}\\n   Price: {product['price']}\\n   Link: {product['link']}\\n\")\n",
    "    else:\n",
    "        print(\"No products found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729fbcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search: guitar\n",
      "Failed to retrieve search results for page 1. Status code: 503\n",
      "No products found.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_products(search_query, pages=3):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    products = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f'https://www.amazon.in/s?k={search_query.replace(\" \", \"+\")}&page={page}'\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve search results for page {page}. Status code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        for item in soup.find_all('div', {'data-component-type': 's-search-result'}):\n",
    "            title = item.h2.text.strip() if item.h2 else '-'\n",
    "            link = f\"https://www.amazon.in{item.h2.a['href']}\" if item.h2 and item.h2.a else '-'\n",
    "            price = item.find('span', 'a-price-whole')\n",
    "            price = price.text if price else '-'\n",
    "            brand = item.find('span', 'a-size-base-plus')\n",
    "            brand = brand.text.strip() if brand else '-'\n",
    "            return_exchange = '-'\n",
    "            expected_delivery = '-'\n",
    "            availability = '-'\n",
    "            \n",
    "            if link != '-':\n",
    "                product_response = requests.get(link, headers=headers)\n",
    "                product_soup = BeautifulSoup(product_response.content, 'html.parser')\n",
    "\n",
    "                return_exchange_tag = product_soup.find('a', {'id': 'RETURNS_POLICY'})\n",
    "                return_exchange = return_exchange_tag.text.strip() if return_exchange_tag else '-'\n",
    "\n",
    "                expected_delivery_tag = product_soup.find('div', {'id': 'ddmDeliveryMessage'})\n",
    "                expected_delivery = expected_delivery_tag.text.strip() if expected_delivery_tag else '-'\n",
    "\n",
    "                availability_tag = product_soup.find('div', {'id': 'availability'})\n",
    "                availability = availability_tag.text.strip() if availability_tag else '-'\n",
    "\n",
    "            products.append({\n",
    "                'Brand Name': brand,\n",
    "                'Name of the Product': title,\n",
    "                'Price': price,\n",
    "                'Return/Exchange': return_exchange,\n",
    "                'Expected Delivery': expected_delivery,\n",
    "                'Availability': availability,\n",
    "                'Product URL': link\n",
    "            })\n",
    "            \n",
    "        time.sleep(2)\n",
    "\n",
    "    return products\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the product to search: \")\n",
    "    products = get_products(search_query)\n",
    "\n",
    "    if products:\n",
    "        df = pd.DataFrame(products)\n",
    "        df.to_csv(f\"{search_query}_amazon_products.csv\", index=False)\n",
    "        print(f\"Data has been saved to {search_query}_amazon_products.csv\")\n",
    "    else:\n",
    "        print(\"No products found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146cf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def search_images(driver, query):\n",
    "    driver.get('https://images.google.com/')\n",
    "    search_box = driver.find_element(By.NAME, 'q')\n",
    "    search_box.send_keys(query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(2)  \n",
    "    return driver.page_source\n",
    "\n",
    "def scrape_urls(html, num_images):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    img_tags = soup.find_all('img', {'class': 'rg_i'}, limit=num_images)\n",
    "    urls = [img['src'] for img in img_tags if 'src' in img.attrs]\n",
    "    return urls\n",
    "\n",
    "def save_images(urls, folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    for i, url in enumerate(urls):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(folder_name, f'image_{i + 1}.jpg'), 'wb') as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    num_images = 10\n",
    "    \n",
    "    for keyword in keywords:\n",
    "        html = search_images(driver, keyword)\n",
    "        image_urls = scrape_urls(html, num_images)\n",
    "        save_images(image_urls, keyword)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3dab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "pip install selenium beautifulsoup4 pandas requests webdriver-manager\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def search_flipkart(driver, query):\n",
    "    driver.get('https://www.flipkart.com/')\n",
    "    time.sleep(2)\n",
    "    close_login_popup(driver)\n",
    "    search_box = driver.find_element(By.NAME, 'q')\n",
    "    search_box.send_keys(query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(2)  \n",
    "    return driver.page_source\n",
    "\n",
    "def close_login_popup(driver):\n",
    "    try:\n",
    "        close_button = driver.find_element(By.XPATH, '//button[contains(text(),\"âœ•\")]')\n",
    "        close_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def scrape_phone_details(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    products = []\n",
    "    \n",
    "    for item in soup.find_all('div', {'class': '_1AtVbE'}):\n",
    "        title_tag = item.find('a', {'class': 'IRpwTa'})\n",
    "        if not title_tag:\n",
    "            continue\n",
    "\n",
    "        title = title_tag.text\n",
    "        link = f\"https://www.flipkart.com{title_tag['href']}\"\n",
    "        details = item.find('div', {'class': '_3k-BhJ'})\n",
    "        price = item.find('div', {'class': '_30jeq3 _1_WHN1'})\n",
    "        price = price.text if price else '-'\n",
    "        \n",
    "        details_text = details.text if details else ''\n",
    "        brand_name = title.split()[0]\n",
    "        \n",
    "        product_details = {\n",
    "            'Brand Name': brand_name,\n",
    "            'Phone name': title,\n",
    "            'Colour': extract_detail(details_text, 'Color'),\n",
    "            'RAM': extract_detail(details_text, 'RAM'),\n",
    "            'Storage(ROM)': extract_detail(details_text, 'Storage'),\n",
    "            'Primary Camera': extract_detail(details_text, 'Primary Camera'),\n",
    "            'Secondary Camera': extract_detail(details_text, 'Secondary Camera'),\n",
    "            'Display Size': extract_detail(details_text, 'Display Size'),\n",
    "            'Battery Capacity': extract_detail(details_text, 'Battery Capacity'),\n",
    "            'Price': price,\n",
    "            'Product URL': link\n",
    "        }\n",
    "        products.append(product_details)\n",
    "\n",
    "    return products\n",
    "\n",
    "def extract_detail(details_text, keyword):\n",
    "    detail_parts = details_text.split('|')\n",
    "    for part in detail_parts:\n",
    "        if keyword in part:\n",
    "            return part.split(':')[1].strip()\n",
    "    return '-'\n",
    "\n",
    "def main():\n",
    "    driver = setup_driver()\n",
    "    search_query = input(\"Enter the phone to search: \")\n",
    "    html = search_flipkart(driver, search_query)\n",
    "    driver.quit()\n",
    "    \n",
    "    products = scrape_phone_details(html)\n",
    "    \n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(f\"{search_query}_flipkart_phones.csv\", index=False)\n",
    "    print(f\"Data has been saved to {search_query}_flipkart_phones.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "pip install selenium webdriver-manager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import re\n",
    "\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def get_coordinates(city):\n",
    "    driver = setup_driver()\n",
    "    driver.get('https://maps.google.com')\n",
    "    time.sleep(2)\n",
    "\n",
    "    search_box = driver.find_element(By.NAME, 'q')\n",
    "    search_box.send_keys(city)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(5)  \n",
    "\n",
    "    url = driver.current_url\n",
    "    driver.quit()\n",
    "    \n",
    "    match = re.search(r'@(-?\\d+\\.\\d+),(-?\\d+\\.\\d+)', url)\n",
    "    if match:\n",
    "        latitude, longitude = match.groups()\n",
    "        return float(latitude), float(longitude)\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the place to search: \")\n",
    "    latitude, longitude = get_coordinates(city)\n",
    "    if latitude and longitude:\n",
    "        print(f\"Coordinates of {city}: Latitude = {latitude}, Longitude = {longitude}\")\n",
    "    else:\n",
    "        print(\"Could not find coordinates for the specified city.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "pip install requests beautifulsoup4 pandas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_gaming_laptops():\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve webpage. Status code: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    laptops = []\n",
    "\n",
    "    laptop_listings = soup.find_all('div', class_='TopNumbeHeading')\n",
    "    \n",
    "    for listing in laptop_listings:\n",
    "        details = listing.find_next('div', class_='Section-center').text.strip().split('\\n')\n",
    "        laptop_details = {\n",
    "            'Name': listing.h2.text.strip() if listing.h2 else '-',\n",
    "            'Specs': ' '.join(details).replace('Read More', '').strip(),\n",
    "        }\n",
    "        laptops.append(laptop_details)\n",
    "    \n",
    "    return laptops\n",
    "\n",
    "def main():\n",
    "    laptops = get_gaming_laptops()\n",
    "    \n",
    "    if laptops:\n",
    "        df = pd.DataFrame(laptops)\n",
    "        df.to_csv(\"gaming_laptops.csv\", index=False)\n",
    "        print(\"Data has been saved to gaming_laptops.csv\")\n",
    "    else:\n",
    "        print(\"No data found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc529d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "pip install google-api-python-client\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY = 'YOUR_API_KEY'\n",
    "VIDEO_ID = 'YOUR_VIDEO_ID'\n",
    "\n",
    "def get_comments(api_key, video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    comments = []\n",
    "    \n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        maxResults=100,\n",
    "        textFormat='plainText'\n",
    "    )\n",
    "\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Upvotes': comment['likeCount'],\n",
    "                'Time Posted': comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        if 'nextPage' in response:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                pageToken=response['nextPage'],\n",
    "                maxResults=100,\n",
    "                textFormat='plainText'\n",
    "            )\n",
    "        else:\n",
    "            request = None\n",
    "\n",
    "        if len(comments) >= 500:\n",
    "            break\n",
    "\n",
    "    return comments[:500]\n",
    "\n",
    "def main():\n",
    "    comments = get_comments(API_KEY, VIDEO_ID)\n",
    "    \n",
    "    if comments:\n",
    "        df = pd.DataFrame(comments)\n",
    "        df.to_csv(\"youtube_comments.csv\", index=False)\n",
    "        print(\"Data has been saved to youtube_comments.csv\")\n",
    "    else:\n",
    "        print(\"No comments found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
